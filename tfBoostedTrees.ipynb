{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   }
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tf.random.set_seed(123)\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "qualified                   0\nisRelay                     0\nclipped_divsRank            0\nnormed_divsSpeed            0\nnormed_divsTimePctOfMean    0\nstroke                      0\npoints                      0\ndtype: int64"
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_dataset = pd.read_csv(\"tier6.csv\")\n",
    "df = raw_dataset.copy()\n",
    "df = df.dropna(subset=[\"divsTime\"])\n",
    "df = df[[\"qualified\", \"isRelay\", \"clipped_divsRank\", \"normed_divsSpeed\", \"normed_divsTimePctOfMean\", \"stroke\", \"points\"]]\n",
    "df.isna().sum()\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "qualified                      bool\nisRelay                        bool\nclipped_divsRank            float64\nnormed_divsSpeed            float64\nnormed_divsTimePctOfMean    float64\nstroke                       object\npoints                        int64\ndtype: object"
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# df.qualified = df.qualified.astype(int)\n",
    "# df.isRelay = df.isRelay.astype(int)\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftrain = df.sample(frac=0.8,random_state=0)\n",
    "dfeval = df.drop(dftrain.index)\n",
    "y_train = dftrain.pop('points')\n",
    "y_eval = dfeval.pop('points')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "column_name: qualified vocabulary dtype must be string or integer. dtype: <dtype: 'bool'>.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-50-df0c3a110e0f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;31m# Need to one-hot encode categorical features.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mvocabulary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mfeature_columns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mone_hot_cat_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfeature_name\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mNUMERIC_COLUMNS\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-50-df0c3a110e0f>\u001b[0m in \u001b[0;36mone_hot_cat_column\u001b[1;34m(feature_name, vocab)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mone_hot_cat_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     return fc.indicator_column(\n\u001b[1;32m----> 7\u001b[1;33m       \u001b[0mfc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcategorical_column_with_vocabulary_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     )\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\tang-\\Documents\\Python Scripts\\skynet2\\skynetvenv\\lib\\site-packages\\tensorflow_core\\python\\feature_column\\feature_column_v2.py\u001b[0m in \u001b[0;36mcategorical_column_with_vocabulary_list\u001b[1;34m(key, vocabulary_list, dtype, default_value, num_oov_buckets)\u001b[0m\n\u001b[0;32m   1792\u001b[0m           num_oov_buckets, key))\n\u001b[0;32m   1793\u001b[0m   fc_utils.assert_string_or_int(\n\u001b[1;32m-> 1794\u001b[1;33m       vocabulary_dtype, prefix='column_name: {} vocabulary'.format(key))\n\u001b[0m\u001b[0;32m   1795\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1796\u001b[0m     \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocabulary_dtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\tang-\\Documents\\Python Scripts\\skynet2\\skynetvenv\\lib\\site-packages\\tensorflow_core\\python\\feature_column\\utils.py\u001b[0m in \u001b[0;36massert_string_or_int\u001b[1;34m(dtype, prefix)\u001b[0m\n\u001b[0;32m     56\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstring\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mnot\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_integer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     raise ValueError(\n\u001b[1;32m---> 58\u001b[1;33m         '{} dtype must be string or integer. dtype: {}.'.format(prefix, dtype))\n\u001b[0m\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: column_name: qualified vocabulary dtype must be string or integer. dtype: <dtype: 'bool'>."
     ]
    }
   ],
   "source": [
    "fc = tf.feature_column\n",
    "CATEGORICAL_COLUMNS = ['stroke', 'qualified', 'isRelay']\n",
    "NUMERIC_COLUMNS = ['clipped_divsRank', 'normed_divsSpeed', 'normed_divsTimePctOfMean']\n",
    "\n",
    "def one_hot_cat_column(feature_name, vocab):\n",
    "    return fc.indicator_column(\n",
    "      fc.categorical_column_with_vocabulary_list(feature_name, vocab)\n",
    "    )\n",
    "\n",
    "feature_columns = []\n",
    "for feature_name in CATEGORICAL_COLUMNS:\n",
    "    # Need to one-hot encode categorical features.\n",
    "    vocabulary = df[feature_name].unique()\n",
    "    feature_columns.append(one_hot_cat_column(feature_name, vocabulary))\n",
    "\n",
    "for feature_name in NUMERIC_COLUMNS:\n",
    "    feature_columns.append(fc.numeric_column(feature_name,\n",
    "                                              dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use entire batch since this is such a small dataset.\n",
    "NUM_EXAMPLES = len(y_train)\n",
    "\n",
    "def make_input_fn(X, y, n_epochs=None, shuffle=True):\n",
    "  def input_fn():\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((X.to_dict(orient='list'), y))\n",
    "    if shuffle:\n",
    "      dataset = dataset.shuffle(NUM_EXAMPLES)\n",
    "    # For training, cycle thru dataset as many times as need (n_epochs=None).\n",
    "    dataset = (dataset\n",
    "      .repeat(n_epochs)\n",
    "      .batch(NUM_EXAMPLES))\n",
    "    return dataset\n",
    "  return input_fn\n",
    "\n",
    "# Training and evaluation input functions.\n",
    "train_input_fn = make_input_fn(dftrain, y_train)\n",
    "eval_input_fn = make_input_fn(dfeval, y_eval, shuffle=False, n_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "  'n_trees': 50,\n",
    "  'max_depth': 3,\n",
    "  'n_batches_per_layer': 1,\n",
    "  # You must enable center_bias = True to get DFCs. This will force the model to\n",
    "  # make an initial prediction before using any features (e.g. use the mean of\n",
    "  # the training labels for regression or log odds for classification when\n",
    "  # using cross entropy loss).\n",
    "  'center_bias': True\n",
    "}\n",
    "\n",
    "est = tf.estimator.BoostedTreesClassifier(feature_columns, **params)\n",
    "# Train model.\n",
    "est.train(train_input_fn, max_steps=100)\n",
    "\n",
    "# Evaluation.\n",
    "results = est.evaluate(eval_input_fn)\n",
    "pd.Series(results).to_frame()"
   ]
  }
 ]
}